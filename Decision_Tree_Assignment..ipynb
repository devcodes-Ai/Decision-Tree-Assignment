{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n"
      ],
      "metadata": {
        "id": "b7iqgYt_LZDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        " A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It models decisions and their possible consequences in a tree-like structure.\n",
        "Each internal node represents a feature (attribute) test.\n",
        "\n",
        "\n",
        "Each branch represents an outcome of the test.\n",
        "\n",
        "\n",
        "Each leaf node represents a class label (in classification) or a numerical value (in regression).\n",
        "\n",
        "\n",
        "In classification, the tree is built by splitting the dataset into subsets based on the most significant attribute at each step. The splitting is done using measures like Gini Impurity or Entropy. The process continues recursively until the stopping criteria are met.\n",
        "Thus, the Decision Tree works by repeatedly partitioning the data and assigning class labels to unseen instances based on the learned splits.\n"
      ],
      "metadata": {
        "id": "Rwb6pUjOLarJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n"
      ],
      "metadata": {
        "id": "dj3nyvddLdCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Gini Impurity:\n",
        " Measures the probability of incorrectly classifying a randomly chosen element.\n",
        " Gini=1−∑i=1npi2Gini = 1 - \\sum_{i=1}^{n} pi^2Gini=1−i=1∑n​pi2​\n",
        " where pipipi​ is the probability of class iii.\n",
        "\n",
        "\n",
        "A Gini of 0 means perfect purity (all elements belong to one class).\n",
        "\n",
        "\n",
        "The lower the Gini, the better the split.\n",
        "\n",
        "\n",
        "Entropy:\n",
        " Measures the amount of disorder or impurity in the dataset.\n",
        " Entropy=−∑i=1npilog⁡2(pi)Entropy = - \\sum{i=1}^{n} pi \\log2(pi)Entropy=−i=1∑n​pi​log2​(pi​)\n",
        "Entropy is 0 when the data is perfectly pure.\n",
        "\n",
        "\n",
        "Higher entropy means more randomness.\n",
        "\n",
        "\n",
        "Impact on Splits:\n",
        " Both Gini and Entropy help the tree choose the “best” feature for splitting. The feature that produces the largest reduction in impurity (highest Information Gain) is chosen for the split.\n"
      ],
      "metadata": {
        "id": "ctpvDvpVLgIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n"
      ],
      "metadata": {
        "id": "P5f-Z_wbLjAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Pre-Pruning (Early Stopping):\n",
        " The tree growth is stopped early during training when a condition is met (e.g., max depth, min samples per leaf).\n",
        "\n",
        "\n",
        "Advantage: Saves time and reduces overfitting by keeping the tree simpler.\n",
        "\n",
        "\n",
        "Post-Pruning:\n",
        " The tree is first fully grown, and then branches are removed that do not improve accuracy significantly.\n",
        "\n",
        "\n",
        "Advantage: Produces a simpler tree with better generalization while still considering all possible splits first.\n"
      ],
      "metadata": {
        "id": "RwKwarLDLkqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n"
      ],
      "metadata": {
        "id": "ySYT_8xmLmla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        " Information Gain (IG) measures how much “information” about the class labels is gained by splitting on a feature.\n",
        "IG(S,A)=Entropy(S)−∑v∈Values(A)∣Sv∣∣S∣×Entropy(Sv)IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v)IG(S,A)=Entropy(S)−v∈Values(A)∑​∣S∣∣Sv​∣​×Entropy(Sv​)\n",
        "S: dataset\n",
        "\n",
        "\n",
        "A: attribute\n",
        "\n",
        "\n",
        "Sv​: subset for value v of attribute A\n",
        "\n",
        "\n",
        "Importance:\n",
        "Higher IG means the split results in purer subsets.\n",
        "\n",
        "\n",
        "Decision Trees use IG (or Gini reduction) to choose the best feature at each step.\n",
        "\n",
        "\n",
        "It ensures the tree makes the most informative splits, improving classification accuracy.\n"
      ],
      "metadata": {
        "id": "eANCgHxKLoPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?"
      ],
      "metadata": {
        "id": "jUyLRweyLsty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Applications:\n",
        "Healthcare: Predicting diseases.\n",
        "\n",
        "\n",
        "Finance: Loan approval, fraud detection.\n",
        "\n",
        "\n",
        "Marketing: Customer segmentation.\n",
        "\n",
        "\n",
        "Manufacturing: Quality control.\n",
        "\n",
        "\n",
        "Education: Predicting student performance.\n",
        "\n",
        "\n",
        "Advantages:\n",
        "Easy to understand and interpret.\n",
        "\n",
        "\n",
        "Handles both categorical and numerical data.\n",
        "\n",
        "\n",
        "Requires little data preprocessing.\n",
        "\n",
        "\n",
        "Limitations:\n",
        "Can easily overfit if not pruned.\n",
        "\n",
        "\n",
        "Unstable—small changes in data may change the tree.\n",
        "\n",
        "\n",
        "Biased toward features with more levels.\n"
      ],
      "metadata": {
        "id": "kD938mVDLyPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Python Program – ● Load the Iris Dataset ● Train a Decision Tree Classifier using the Gini criterion ● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "GbOF39V3L1sb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_l9ubuGK9Fl",
        "outputId": "a3416a28-deeb-4086-d057-0a188e293b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances: [0.01333333 0.         0.56405596 0.42261071]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train Decision Tree with Gini\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Predictions & Accuracy\n",
        "y_pred = clf.predict(X)\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Python Program – Max Depth Comparison\n",
        "● Load the Iris Dataset ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "PqDxs_CwMDit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fully grown tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "acc_full = clf_full.score(X_test, y_test)\n",
        "\n",
        "# Limited depth tree\n",
        "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "acc_pruned = clf_pruned.score(X_test, y_test)\n",
        "\n",
        "print(\"Fully grown tree accuracy:\", acc_full)\n",
        "print(\"Max depth=3 tree accuracy:\", acc_pruned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXeXWaqtMGVX",
        "outputId": "dc339be9-92ca-43a6-f039-560ddbbb3bac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully grown tree accuracy: 1.0\n",
            "Max depth=3 tree accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to: ● Load the California Housing dataset from sklearn ● Train a Decision Tree Regressor ● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "5-df5tEUMKfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Feature Importances:\", reg.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n5eFlzTMNle",
        "outputId": "b3bcf367-f0fc-4dd8-9796-8db2fe29d82b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.495235205629094\n",
            "Feature Importances: [0.52850909 0.05188354 0.05297497 0.02866046 0.03051568 0.13083768\n",
            " 0.09371656 0.08290203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to: ● Load the Iris Dataset ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV ● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "K3Up8m0QMTTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris(as_frame=True)\n",
        "X = iris.data\n",
        "y = iris.target   # <-- categorical labels (0,1,2)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# GridSearch with DecisionTree\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = grid.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of the best model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb3gFxX9MVv2",
        "outputId": "05beab95-db1a-4501-f1a8-5f22b19c9e1f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Accuracy of the best model: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to: ● Handle the missing values ● Encode the categorical features ● Train a Decision Tree model ● Tune its hyperparameters ● Evaluate its performance And describe what business value this model could provide in the real-world setting."
      ],
      "metadata": {
        "id": "bgrLxiTZMzq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Step 1: Handle Missing Values\n",
        "For numerical features: use mean/median imputation.\n",
        "\n",
        "\n",
        "For categorical features: use mode imputation or “unknown” category.\n",
        "\n",
        "\n",
        "Step 2: Encode Categorical Features\n",
        "Use One-Hot Encoding or Label Encoding depending on the variable type.\n",
        "\n",
        "\n",
        "Step 3: Train Decision Tree Model\n",
        "Split the dataset into training and testing sets.\n",
        "\n",
        "\n",
        "Train a Decision Tree Classifier with proper criteria (Gini/Entropy).\n",
        "\n",
        "\n",
        "Step 4: Hyperparameter Tuning\n",
        "Use GridSearchCV to tune parameters like max_depth, min_samples_split, and criterion.\n",
        "\n",
        "\n",
        "Step 5: Evaluate Performance\n",
        "Use metrics such as Accuracy, Precision, Recall, F1-score, and ROC-AUC.\n",
        "\n",
        "\n",
        "Business Value:\n",
        "The model helps doctors identify high-risk patients quickly.\n",
        "\n",
        "\n",
        "Enables early diagnosis and treatment, improving patient outcomes.\n",
        "\n",
        "\n",
        "Helps reduce costs by focusing medical resources on high-risk cases.\n"
      ],
      "metadata": {
        "id": "cCiy17MTM3ik"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mx55R3L9M2Vb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}